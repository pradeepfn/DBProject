\section{Introduction}
%what is the problem

Managing persistent data has been a challenge since the very early days of computing.
Community have tackled the persistent data challenge via system program abstractions
such as file-systems, data-base management systems, kv-stores, etc.
The non-volatile memory technology solutions such as Intel's 3DXpoint is about to
enter the commodity market. These new devices offer byte addressable persistency
with low latency reads and writes.

Systems community was quick to adopt NVM device properties in their system software
designs. There has been NVM optimized file-systems,databases,kv-stores and programming abstractions.
However, the existing persistent memory programming abstractions in the form the kv-stores, posix 
file APIs and SQL interfaces do not provide data reliability as a first class system
property. That is, as the NVM modules store data local to the machine, there is a possibility of 
losing data in an event of catastrophic machine failure. The existing systems provide reliablity
via data-replication, but such solution are more of an after thought.

Furthermore, the state of the art persistent memory storage abstractions such as DBMSs file-systems
follow the traditional wisdom of storing persistent data with different data layouts/formats
than the application structures that they represent. This scenario is commonly known as 
the impedence mismatch of volatile and peristent data. The reason for such mismatch stems 
from mainly due to their storage device characteristics. The persistent devices used to be
block addressabilty while the main memory is byte addressable.

With ever improving network hardware and low latency network systems software stacks, 
the data replication costs have reduced signficantly over the years. Combined with NVM's
fast stores, it is now possible provide reliable persistent storage with minimal impedence
mismatch. We design and implement Blizzard, a set of programming abstractions that provides 
reliable and persistent data storage. First, Blizzard bridges volatile and persistent impedence
mismatch by directly manipulating persistent memory resident data-structures. Second, it helps
persistent storage programming by supporting well-known C++ STL like persistent containers.
Third, Blizzard improves the reliability of these persistent containers using NVM aware replication library.


\section{Motivation}
%why is this problem important
Non volatile memory(NVM) is an emerging storage technology that promises byte addressable 
persistency. NVMs have read/write latencies that are comparable to DRAM and the device 
writes are durable. Fast accesses latencies and byte addressablity enables this new device
to be placed alongside with DRAM main memory, thus providing direct persistent load/stores
from the processor. Furthermore NVMs have high form factor (high capacity devices) thus
making them ideal devices for both persistent index and data storage. 


First, some of the assumptions that were made during the design of disk oriented persistent
indexes does not hold true anymore, 1) We can access NVM resident data at byte granularity similar to
DRAM data acesses. 2) The NVMs provide load store accesses at DRAM speed. Therefore, we can
possibly store index, alongside the actual data as the index retrival is not slowed down by
accompanying data. Furthermore, the use of DRAM buffer-cache is redundant as the NVMs are 
as good as DRAM in terms of access latency.


The traditional database system uses volatile memory to store lists, trees and maps, but there exists 
impedance between volatile and persistent storage. Data will be lost if node on local NVM fails. The 
motivation of this project is we can sacrify communication overhead to gain data persistence on the 
local memory storage. Though this mechanism adds more overhead to the database system, with the improved
network fabrics, we believe the communication overhead will be negligible in our system. 

The initial project idea is to make the tree data structure persistent and optimize the persistency procedure, 
but we found out that the PMDK implementation does the similar optimizations under the hood. For later 
project, we will focus on making the actual data itself persistent. 

\section{}
%proposed solution

We propose NVStore, a persistent key-value store that is based on NVM aware persistent index
structure. 

\subsection{Zero-copy Index}

First, we store NVStore key-value pairs in a log-structured memory segments on NVM. These, are 
formatted in to C like memory structures, that gets sequentially stored on the log-segments. The
C-structure based key-value storage allows direct referencing/de-referencing of stored data 
as regular memory structures. The log-structuring of groups of such structures allow us to 
exploit the superior sequential performance of the NVM device.

Next, offload some of the metadata maintainance in to data nodes themselves. This is a viable
option becauseo of low latency random accesses supported by the NVM -- a design choice that
is not possible if the random access is slow. In the new design only maintain peristent pointers
in to the corresponding key value ( stored in key-value struct). Thus the internal nodes of the 
NVStore only consists of pointers to keys, removing redundant key data in the index. 
See ~\autoref{fig:zerocopy} for graphical representation of the above idea using a RBTree internal
node.

\begin{figure}[]   
	\centering
	\includegraphics[width=\linewidth]{figures/design.pdf} 
	\caption{\bf Node structure of a RBTree node. We do not store key vlaues in the internal nodes. We simply
	store a persistent pointer in to the external key.} 
	\label{fig:zerocopy} 
\end{figure}

\subsection{Crash-consistent Index Updates}
NVStore index structure is tree consists of key and value pointers that are always of 8 bytes in size
thanks to our 'store only pointers' policy. Therefore, we plan to implement copy-on-write based 
crash-consistent updates. For new udpates, we first create a new subtree that consist of the nodes
that are going to be modified on NVM. Next we persist the newly updated subtree on NVM by careful
usage of cache-flushes and memory fences. Finally, we patch the orginal tree with our new subtree
using a atomic swap and cache-flush, atomically switching index in to newly udpated state.

Above single word atomic swap algorith has one drawback. It tends to create more copies during
COW updates as it has to create new subtree that can plug in to the original index tree with just 
single atomic writes. Recent multiword atomic update primitive\cite{pmwcas} for NVM allows multiword 
atomic updates. We plan to use this primitve to cut down COW costs during crash-cosnsitent index updates.




